{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classifying Survivors: An Exercise in Machine Learning with Python\n**Michael Sieviec**\n\n**August 8, 2019**\n\n# Overview\nThis point of this notebook is to explore data cleaning, analysis, visualization, engineering, and classification algorithms with the Titanic dataset on Kaggle. I found [Titanic Top 4% with ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling) by **Yassine Ghouzam, PhD** to be a helpful resource."},{"metadata":{},"cell_type":"markdown","source":"# A look at the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom statistics import mode\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nfull_data = pd.concat([train.drop(columns = 'Survived'), test], \n                       axis = 0, \n                       sort = False)\n\nfull_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data.shape, train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see the data is fairly slim: there are only 11 variables and just over 1300 data points, nearly 900 of which come from the training set. Having a smaller test set is fortunate as it gives us more data to build our models on."},{"metadata":{"trusted":false},"cell_type":"code","source":"len(full_data['PassengerId'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additionally, the `PassengerId` column is fully unique and thus useless to us. `Name` and `Ticket` also appear troublesome, but they could be of use yet."},{"metadata":{},"cell_type":"markdown","source":"# Missing Data\nBefore we continue, we want to eliminate whatever missing values we can."},{"metadata":{"trusted":false},"cell_type":"code","source":"## missing\nfull_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That is quite a few missing in `Cabin`. `Age`, `Fare`, and `Embarked` will be easy: we will fill the first two by the means as grouped by `Sex` and `Pclass`."},{"metadata":{"trusted":false},"cell_type":"code","source":"# fill age by means of sex, class\nfull_data[['Age','Fare']] = (full_data\n                    .groupby(['Sex', 'Pclass'])[['Age','Fare']]\n                    .transform(lambda x: \n                        x.fillna(round(np.mean(x)))))\n\nfull_data['Embarked'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Embarked` we will simply fill with the most common value as there are only 3 choices."},{"metadata":{"trusted":false},"cell_type":"code","source":"# impute most common port\nfull_data['Embarked'] = (full_data.Embarked\n                         .fillna(mode(full_data.Embarked)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This leaves `Cabin`, but we will be doing something a little different as seen in the next section."},{"metadata":{},"cell_type":"markdown","source":"# New Variables\nWe have some problem variables, specifically `Cabin`, `Name`, and `Ticket`."},{"metadata":{},"cell_type":"markdown","source":"### `Deck`"},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data['Cabin'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of variables in `Cabin`, but they're not entirely unique and there is a pattern. Nearly every entry is one letter from A-G (and one T) followed by 2 numbers. After referencing some images ([1](https://ssmaritime.com/Titanic-3.htm), [2](https://commons.wikimedia.org/wiki/File:Titanic_cutaway_diagram.png#/media/File:Titanic_cutaway_diagram.png)) of the layout of the Titanic, I assume that the letters represent a deck and the numbers represent a cabin. It is concerning that there are more than 1000 missing values, but we will simply mark these entries with an X and hope the information gained is valuable. A new variable, `Deck`, will consist of the last letter in each string."},{"metadata":{"trusted":false},"cell_type":"code","source":"# deck (if any)\nfull_data['Deck'] = (full_data.Cabin\n                     .str.extract('([A-Z])(?=\\d*$)', \n                                  expand = True)\n                     .fillna('X'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Cabin`\nWe're also going to extract the last series of numbers in each string, just in case they are somehow useful, and assign them back to `Cabin`. 0s will fill the empty values."},{"metadata":{"trusted":false},"cell_type":"code","source":"# cabin letter (if any)\nfull_data['Cabin'] = (full_data.Cabin\n                      .str.extract('(\\d+)$', expand = True)\n                      .fillna(0))\nfull_data['Cabin'] = full_data['Cabin'].apply(int) # str to int","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Title`\nIn the similarly intimidating `Name` variable, there are discernible reoccurances. Particularly, there appear to be titles, e.g. Mr., Mrs, and so forth. After digging through the names, I found a few that appeared to occur frequently enough to warrant their own designations, as well as some that seemed to warrant aggregation into either an 'Officer' or 'Other' category."},{"metadata":{"trusted":false},"cell_type":"code","source":"# title\ntitle_extract = ('(Mr\\.|Master|Mrs\\.|Miss\\.|Rev\\.|Capt\\.|Col\\.|Major\\.)')\ntitle_dict = {'Capt\\.|Major\\.|Col\\.' : 'Officer'}\nfull_data['Title'] = (full_data.Name\n                      .str.extract(title_extract)\n                      .replace(title_dict, \n                               regex = True)\n                      .fillna('Other'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data.groupby('Title').size()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That looks a lot more manageable than `Name`."},{"metadata":{},"cell_type":"markdown","source":"### `TicketNumber`\nLess frequently explored seems to be the `Ticket` variable. It is understandable, as it's arguably the messiest of all the variables. However, I did catch on to one possible pattern."},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data['Ticket'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There appears to be a number in every entry. I don't know that this will be useful yet, but we have precious little information so I'm going to extract these anyway and hope they come in handy. There also do appear to be some patterns in the lettering, but they aren't uniform enough for me to want to explore them."},{"metadata":{"trusted":false},"cell_type":"code","source":"# ticketnumber\nfull_data['TicketNumber'] = (full_data.Ticket\n                             .str.extract('(\\d+)$', expand = True))\nfull_data.loc[full_data.TicketNumber.notnull(), 'TicketNumber'] = (full_data\n                                                                   .loc[full_data.TicketNumber.notnull(),\n                                                                        'TicketNumber']\n                                                                   .astype(int)) # str to int\n\n# 4 missing tickets\n(full_data.Ticket\n .str.extract('(\\d+)$', expand = True)\n .isnull()\n .sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we finally impute these missing ticket numbers from the data as grouped by `Pclass` and `Embarked`."},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data['TicketNumber'] = (full_data\n                             .groupby(['Pclass', 'Embarked'])['TicketNumber']\n                             .transform(lambda x: x.fillna(np.mean(x))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizations\nThe data is now clean enough to have a look at graphically. We're looking for insight into what makes someone more likely to survive."},{"metadata":{"trusted":false},"cell_type":"code","source":"# new training set for probabilities and visualizations\ntrain_for_vis = pd.concat([train.Survived, full_data[:train.shape[0]]], axis = 1)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_palette('cool')\n\n# without family more likely to die\nsns.catplot(x = 'Parch', \n            y = 'Survived', \n            data = train_for_vis,\n            height = 5,\n            aspect = 1.5,\n            kind = 'bar')\nplt.title('Figure 1: Survival by Number of Parents/Children')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It appears that people travelling in small families faired better than either those who were alone or with large families."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.catplot(x = 'SibSp', \n            y = 'Survived', \n            data = train_for_vis,\n            height = 5,\n            aspect = 1.5,\n            kind = 'bar')\nplt.title('Figure 2: Survival by Number of Siblings/Spouses')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pattern holds for those with siblings and spouses."},{"metadata":{"trusted":false},"cell_type":"code","source":"# survival by deck\nsns.catplot(x = 'Deck', \n            y = 'Survived', \n            data = train_for_vis, \n            kind = 'bar',\n            height = 5,\n            aspect = 1.85,\n            order = sorted(list(train_for_vis['Deck'].unique())))\nplt.title('Figure 3: Survival by Deck')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Deck` turned out to be insightful: those on decks marked A, G, or T, or those without a marked deck (X) all fared significantly worse than those on others, with passengers on deck C falling somewhere in between."},{"metadata":{"trusted":false},"cell_type":"code","source":"# survival by sex\nsns.catplot(x = 'Sex', \n            y = 'Survived', \n            data = train_for_vis,\n            kind = 'bar')\nplt.title('Figure 4: Survival by Sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the men were really out of luck on this trip."},{"metadata":{"trusted":false},"cell_type":"code","source":"# survival by age\ng = sns.FacetGrid(train_for_vis, \n                  hue = 'Survived',\n                  height = 5,\n                  aspect = 1.5,\n                  palette = 'cool')\ng.map(sns.kdeplot, 'Age', shade = True).add_legend()\nplt.title('Figure 5: Survival by Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here's a compelling figure: it shows that there are pretty clear discrepancies in survival rates based on age groups. We will make use of them."},{"metadata":{"trusted":false},"cell_type":"code","source":"g = sns.FacetGrid(train_for_vis, \n                  hue = 'Survived',\n                  height = 5,\n                  aspect = 1.5,\n                  palette = 'cool')\ng.map(sns.kdeplot, 'TicketNumber', shade = True).add_legend()\nplt.title('Figure 6: Survival by TicketNumber')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And another: there appear to be 3 distinct classes of survival rates in the ticket numbers. We'll make use of this, as well."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"g = sns.FacetGrid(train_for_vis, \n                  hue = 'Survived',\n                  height = 5,\n                  aspect = 1.5,\n                  palette = 'cool')\ng.map(sns.kdeplot, 'Cabin', shade = True).add_legend()\nplt.title('Figure 7: Survival by Cabin Number')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I don't know how useful this one is going to be, but let's keep exploring."},{"metadata":{},"cell_type":"markdown","source":"### Correlation\nCorrelation is one method of discerning if a variable has a relationship with another or not. For our purposes, we're let's look at our variables' correlation with survival."},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_map = train_for_vis.corr(method = 'pearson')\ncorr_map = (corr_map\n            .transform(lambda x: np.flip(x, 0), \n                       axis = 0))\nmask = np.tri(corr_map.shape[0], k = -1).T\n\nplt.figure(figsize = (12, 9))\nsns.heatmap(corr_map, \n            annot = True, \n            cmap = 'GnBu_r', \n            mask = np.flip(mask, axis = 1),\n            vmin = 0,\n            vmax = 1)\nplt.title(\"Figure 8: Pearson Correlation Coefficient\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, `PassengerId` is not useful. Interestingly, `TicketNumber` is slightly more correlated than `Age` is. And actually, so is `Cabin`, so I guess we're going to hang on to that one.\n\nHowever, the Pearson correlation coefficient only detects linear relationships between the variables. These variables may not be linearly related. Let's see if Spearman's rho is any more enlightening."},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_map = train_for_vis.corr(method = 'spearman')\ncorr_map = (corr_map\n            .transform(lambda x: np.flip(x, 0), \n                       axis = 0))\nmask = np.tri(corr_map.shape[0], k = -1).T\n\nplt.figure(figsize = (12, 9))\nsns.heatmap(corr_map, \n            annot = True, \n            cmap = 'GnBu_r', \n            mask = np.flip(mask, axis = 1),\n            vmin = 0,\n            vmax = 1)\nplt.title(\"Figure 9: Spearman's Rho\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlations for the relavant variables are all a little stronger when we look at things non-linearly. Not astoundingly so, but better at least."},{"metadata":{},"cell_type":"markdown","source":"# Variable Engineering\nThere are still some things we can do to improve the predicting power of our variables. For my own curiosity, I'm going to explore ordinal categorization of our variables as much as is reasonable (and maybe a little that isn't, but hey, messing around is fun). The idea behind this is that besides things like your title or accomodations just being a factor in your survival, we can discern mathematically the chances of your survival when each of these is taken individually. If you sort these chances by order&mdash;voilà!&mdash;you have ordinal or qualitative data. This may not be the optimal approach but it sounds fun and makes enough sense to me, so I'm doing it."},{"metadata":{},"cell_type":"markdown","source":"### `Sex`\nFirst, we make sex a binary variable ordered based on the fact that women clearly had better survival outcomes than men."},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data['Sex'] = (full_data['Sex']\n                    .map({'male' : 0, 'female' : 1}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `TicketCat`\nNext, we categorize `TicketNumber` based on the survival discrepancies from the earlier graph."},{"metadata":{"trusted":false},"cell_type":"code","source":"def ticket_transform(data):\n    if data < 200000:\n        data = 0\n    elif 200000 <= data < 1000000:\n        data = 1\n    else:\n        data = 2\n    return data\n\nfull_data['TicketCat'] = (full_data['TicketNumber']\n                          .transform(ticket_transform))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we actually order them by survival probabilities."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_for_vis = pd.concat([train.Survived, full_data[:train.shape[0]]], axis = 1)\n\n# make ordinal\nticket_by_prob = (train_for_vis\n                 .groupby(['TicketCat'])['Survived']\n                 .agg('mean')\n                 .sort_values(ascending = True)\n                 .index)\nticket_map = dict(zip(ticket_by_prob,\n                    [x for x in range(len(ticket_by_prob))]))\nfull_data['TicketCat'] = full_data['TicketCat'].map(ticket_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(train_for_vis\n    .groupby(['TicketCat'])['Survived']\n    .agg('mean')\n    .sort_values(ascending = True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the category of highest ticket numbers had the worst survival outcomes by a small margin.\n\nWe proceed in a similar manner for other variables.\n\n###  `AgeCat`\n`Age` as categorized by survival."},{"metadata":{"trusted":false},"cell_type":"code","source":"def age_transform(data):\n    if data < 17:\n        data = 0\n    elif 17 <= data < 32:\n        data = 1\n    elif 32 <= data < 42:\n        data = 2\n    elif 42 <= data < 60:\n        data = 3\n    else:\n        data = 4\n    return data\n\nfull_data['AgeCat'] = (full_data['Age']\n                       .transform(age_transform))\n# make ordinal\ntrain_for_vis = pd.concat([train.Survived, full_data[:train.shape[0]]], axis = 1)\nage_by_prob = (train_for_vis\n                 .groupby(['AgeCat'])['Survived']\n                 .agg('mean')\n                 .sort_values(ascending = True)\n                 .index)\nage_map = dict(zip(age_by_prob,\n                    [x for x in range(len(age_by_prob))]))\nfull_data['AgeCat'] = full_data['AgeCat'].map(age_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `FamilyCat`\nAnd family size category."},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data['FamilySize'] = full_data['SibSp'] + full_data['Parch']\n\nfull_data['FamilyCat'] = (full_data['FamilySize']\n                          .map({0 : 'Alone',\n                                **dict.fromkeys([1, 2, 3], 'Small'),\n                                **dict.fromkeys([4, 5, 6, 7, 8, 9, 10], 'Large')}))\n\ntrain_for_vis = pd.concat([train.Survived, full_data[:train.shape[0]]], axis = 1)\n\n# ordinal\nfam_by_prob = (train_for_vis\n                 .groupby(['FamilyCat'])['Survived']\n                 .agg('mean')\n                 .sort_values(ascending = True)\n                 .index)\nfam_map = dict(zip(fam_by_prob,\n                    [x for x in range(len(fam_by_prob))]))\nfull_data['FamilyCat'] = full_data['FamilyCat'].map(fam_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Title`"},{"metadata":{"trusted":false},"cell_type":"code","source":"# ordinal title\ntitle_by_prob = (train_for_vis\n                 .groupby(['Title'])['Survived']\n                 .agg('mean')\n                 .sort_values(ascending = True)\n                 .index)\n\ntitle_map = dict(zip(title_by_prob,\n                     [x for x in range(len(title_by_prob))]))\nfull_data['Title'] = full_data['Title'].map(title_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Pclass`"},{"metadata":{"trusted":false},"cell_type":"code","source":"# ordinal class\nclass_by_prob = (train_for_vis\n                 .groupby(['Pclass'])['Survived']\n                 .agg('mean')\n                 .sort_values(ascending = True)\n                 .index)\nclass_map = dict(zip(class_by_prob,\n                     [x for x in range(len(class_by_prob))]))\nfull_data['Pclass'] = full_data['Pclass'].map(class_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Embarked`"},{"metadata":{"trusted":false},"cell_type":"code","source":"# ordinal port\nport_by_prob = (train_for_vis\n                 .groupby(['Embarked'])['Survived']\n                 .agg('mean')\n                 .sort_values(ascending = True)\n                 .index)\nport_map = dict(zip(port_by_prob,\n                    [x for x in range(len(port_by_prob))]))\nfull_data['Embarked'] = full_data['Embarked'].map(port_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### `Deck`"},{"metadata":{"trusted":false},"cell_type":"code","source":"# ordinal deck\ndeck_by_prob = (train_for_vis\n                 .groupby(['Deck'])['Survived']\n                 .agg('mean')\n                 .sort_values(ascending = True)\n                 .index)\ndeck_map = dict(zip(deck_by_prob,\n                    [x for x in range(len(deck_by_prob))]))\nfull_data['Deck'] = full_data['Deck'].map(deck_map)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we dispose of the old variables in favor of the re-engineered ones."},{"metadata":{"trusted":false},"cell_type":"code","source":"to_drop_cols = ['PassengerId', 'Name', 'Age',\n                'SibSp', 'Parch', 'Ticket', \n                'TicketNumber', 'FamilySize']\n\nfull_data = full_data.drop(columns = to_drop_cols)\nfull_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's looking much easier to deal with, but we've got a little further to go."},{"metadata":{},"cell_type":"markdown","source":"# Outliers and Insight\nOutliers can mess up any model, so let's see if we can't find any in a pair plot. We're also going to take this time to inspect each variable combination to discern if there is some kind of clear classification boundary between those who survived and those who didn't in order to guide our model selection."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean = pd.concat([train['Survived'], \n                         full_data[:train.shape[0]]], axis = 1)\nsns.pairplot(train_clean, \n             hue = 'Survived', \n             vars = list(train_clean.drop(columns = 'Survived').columns))\nplt.suptitle('Figure 10: Pairplot of All Variables', y = 1.02)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though the scale is difficult to work with, there doesn't appear to be a clear boundary between those who survived and those who didn't in any of the plots, so I won't employ discriminant analysis. We do, however, see some outliers in the `Fare`/`Cabin` graph: the highest fares are way above all of the others. Those should be easy to find and be rid of."},{"metadata":{"trusted":false},"cell_type":"code","source":"to_drop_rows = [343, 679, 258, 737] # high fares","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization\n`Fare` and `Cabin` are *heavily* skewed, we can tell just by the exploratory graphs above, so we're going to log(1+x) transform them so that they're a little more normal because we want to use at least one generalized linear model."},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data[['Cabin', 'Fare']] = (full_data[['Cabin', 'Fare']]\n                                .transform(lambda x: np.log(1+x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"full_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now we reassemble our training and testing sets."},{"metadata":{"trusted":false},"cell_type":"code","source":"train_clean = pd.concat([train['Survived'], \n                         full_data[:train.shape[0]]], axis = 1)\ntrain_clean = train_clean.drop(index = to_drop_rows)\ntest_clean = full_data[(train_clean.shape[0] + len(to_drop_rows)):]\ntrain_clean.shape, test_clean.shape  # minus 4 outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at a correlation matrix with our new variables."},{"metadata":{"trusted":false},"cell_type":"code","source":"corr_map = train_clean.corr(method = 'spearman')\ncorr_map = (corr_map\n            .transform(lambda x: np.flip(x, 0), \n                       axis = 0))\nmask = np.tri(corr_map.shape[0], k = -1).T\n\nplt.figure(figsize = (12, 9))\nsns.heatmap(corr_map, \n            annot = True, \n            cmap = 'GnBu_r', \n            mask = np.flip(mask, axis = 1),\n            vmin = 0,\n            vmax = 1)\nplt.title(\"Figure 11: Spearman's Rho\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our new variables seem to be better predictors of `Survived` than the old. Notice how highly correlated `Title`/`Sex` and `Cabin`/`Deck` are. The first relationship is no surprise&mdash;in those days, your title would be in direct relation to your sex. The second is a little more compelling, as `Cabin` became the cabin numbers, and `Deck` the cabin letters by increasing order of survival. This seems to mean that as your chances of your living on a better deck in terms of survival increased, the chances of you having a higher cabin number did, also, and significantly so. There *were* a lot of zeros imputed into `Cabin`. I would not be surprised to find that the absense of a cabin number in the first place were not a mistake, and that many simply didn't have accomodations considered as a cabin."},{"metadata":{},"cell_type":"markdown","source":"# Modeling\nNow we explore a few models. I won't be doing ensembling (at least, not meta-ensembling), but simply sticking to some methods I wanted to get more comfortable with as well as explore one or two that I'm unfamiliar with.\n\nLet's set up the shop:"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom collections import namedtuple\nfrom sklearn.utils import shuffle\n\nshuffled = shuffle(train_clean, random_state = 42)\npredictors = shuffled.drop(columns = 'Survived')\nresponse = shuffled['Survived']\n\n# accuracy assessment\ndef get_accuracy(model, resp):\n    out = namedtuple('Output', 'Accuracy Predictions')\n    pred = cross_val_predict(model, predictors, resp, cv = 10)\n    acc = accuracy_score(resp, pred)\n    return out(acc, pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A function for assessing the model out-of-sample performance was defined based on a cross-validation loop, and the training data shuffled because some of the classifiers we will use take subsets of the training data at each step, so we wanted to break up any patterns in the data that may have been present in any given chunk(s)."},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression\nThis one is a (modern) classic. It is a generalized linear model that maximizes a [log-likelihood function](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) to assign probabilities to each observation. While comparatively simple to some other methods, I once tried to program it from scratch for a school project and found it fantastically difficult. Still, the nuts and bolts are easy to understand: if the probability for an observation is < 0.5, a 0 is predicted&mdash;else, a 1.\n\nNote: somewhat extensive parameter tuning was done with most of the models which I will not reiterate here."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel_lr = LogisticRegression(C = 1.75,\n                              solver = 'liblinear',\n                              fit_intercept = True, \n                              max_iter = 10000)\nacc_lr = get_accuracy(model_lr, response)\nprint(f'Logistic Regression classification accuracy was {acc_lr.Accuracy}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a relatively traditional model, that's not bad at all. It's not great, but it's not bad."},{"metadata":{"trusted":false},"cell_type":"code","source":"model_lr.fit(predictors, response)\nprint('Logistic Regression Coefficients')\nfeatures = pd.DataFrame(dict(zip(list(train_clean.drop(columns = 'Survived')), \n                      list(model_lr.coef_[0]))), index = range(1)).sort_values(by = 0, axis = 1, ascending = False)\nsns.catplot(data = features,\n            kind = 'bar',\n            height = 5,\n            aspect = 1.5,\n            palette = 'magma_r')\nplt.title('Figure 12: Logistic Regression Coefficients')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see logistic regression more positively associated `Class`, `Title`, and `FamilyCat` with survival."},{"metadata":{},"cell_type":"markdown","source":"# Gaussian Process Classification\nThis one caught my eye in the sklearn catalog. After reading some of [this book excerpt](http://www.gaussianprocess.org/gpml/chapters/RW3.pdf) (Rasmussen and Williams) and learning it is a more generalized version of logistic regression, I decided I'd try it."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.gaussian_process import GaussianProcessClassifier\nmodel_gpc = GaussianProcessClassifier(n_restarts_optimizer = 10,\n                                      max_iter_predict = 100000,\n                                      random_state = 5)\nacc_gpc = get_accuracy(model_gpc, response)\nprint(f'Gaussian Process classification accuracy was {acc_gpc.Accuracy}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unsurprisingly, it does a little better than traditional logistic regression given it's more general nature."},{"metadata":{},"cell_type":"markdown","source":"# K-Nearest Neighbors\nThis one is another classic of the classifiers. It relies on Euclidean distance by assigning the mode class of the k-nearest neighbors to an unclassified data point. Sklearn also incorporates some more complex methods to help cut down on computation time (as this process can be quite expensive with larger datasets)."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nmodel_kn = KNeighborsClassifier(n_neighbors = 9,\n                                n_jobs = 2)\nacc_kn = get_accuracy(model_kn, response)\nprint(f'K-Nearest Neighbors classification accuracy was {acc_kn.Accuracy}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting a little better still.\n\n# Support Vector Classification\n[A Practical Guide to Support Vector Classiﬁcation](https://www.researchgate.net/profile/Chenghai_Yang/publication/272039161_Evaluating_unsupervised_and_supervised_image_classification_methods_for_mapping_cotton_root_rot/links/55f2c57408ae0960a3897985/Evaluating-unsupervised-and-supervised-image-classification-methods-for-mapping-cotton-root-rot.pdf) (Hsu et. al.) does a better job of explaining how these work than I probably ever will."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.svm import SVC\nmodel_svc = SVC(max_iter = 1000000, \n                C = 0.5,\n                kernel = 'rbf',\n                gamma = 'scale',\n                probability = True,\n                random_state = 5)\nacc_svc = get_accuracy(model_svc, response)\nprint(f'Support Vector classification accuracy was {acc_svc.Accuracy}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost\nXGBoost is a boosted trees algorithm. They do a great job of explaining the inner workings of them [here](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)."},{"metadata":{"trusted":false},"cell_type":"code","source":"import xgboost as xgb\nmodel_xgb = xgb.XGBClassifier(reg_alpha = .35,\n                              reg_lambda = .6,\n                              nthread = 2,\n                              seed = 1)\nacc_xgb = get_accuracy(model_xgb, response)\nprint(f'XGBoost classification accuracy was {acc_xgb.Accuracy}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_xgb.fit(predictors, response)\nmodel_xgb.feature_importances_\nfeatures = pd.DataFrame(dict(zip(list(train_clean.drop(columns = 'Survived')), \n                      list(model_xgb.feature_importances_))), index = range(1)).sort_values(by = 0, axis = 1, ascending = False)\nsns.catplot(data = features,\n            kind = 'bar',\n            height = 5,\n            aspect = 1.5,\n            palette = 'magma_r')\nplt.title('Figure 13: XGBoost Feature Importances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The three most important features are the same for XGBoost as the logit model."},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\nRandom forest is a very popular machine learning technique based on the aggregation of decision trees in a manner similar but not identical to XGBoost."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(random_state = 5,\n                                  max_depth = 6,\n                                  max_features = 8,\n                                  n_estimators = 500,\n                                  n_jobs = 2)\nacc_rf = get_accuracy(model_rf, response)\nprint(f'Random forest classification accuracy was {acc_rf.Accuracy}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model_rf.fit(predictors, response)\nmodel_rf.feature_importances_\nfeatures = pd.DataFrame(dict(zip(list(train_clean.drop(columns = 'Survived')), \n                      list(model_rf.feature_importances_))), index = range(1)).sort_values(by = 0, axis = 1, ascending = False)\nsns.catplot(data = features,\n            kind = 'bar',\n            height = 5,\n            aspect = 1.5,\n            palette = 'magma_r')\nplt.title('Figure 14: Random Forest Feature Importances')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As with XGBoost, random forest found `Title` to be the most important feature. Interestingly, it rounded out the top three most important features with `Fare` and `Sex`, unlike the others we were able to visualize, and it performed the best by a small margin."},{"metadata":{},"cell_type":"markdown","source":"# Summary\nThe data required a fair bit of cleaning and engineering, but it was ultimately very usable, which is a testament to being creative in your approach. For prediction, random forest (unsurprisingly) did the best, clocking in at greater than 84% accuracy during cross-validation. Interestingly, feature coefficients and importance fell not very far from our Spearman's rho correlation, so it may be a good starting point for feature selection and engineering after all. Seeing what others have done, this seems to be on par with the better results for individual algorithms. As others have done, one might employ meta modeling to improve accuracy."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}